{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11dc6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config SqlMagic.autopandas = True\n",
    "%config SqlMagic.feedback = True\n",
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca5a024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcdad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データベースに接続\n",
    "conn = sqlite3.connect('tataki.sqlite3')\n",
    "curs = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20aac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_sql_query('SELECT * FROM tataki', conn)\n",
    "# print(df)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0835481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# カーソルとコネクションをクローズする\n",
    "curs.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbe43c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# // array(1次元)をtargetLengsの長さにする\n",
    "def getArrayLengsChange(array, targetLengs):\n",
    "    targetArray = [0] * targetLengs\n",
    "    if len(array) == targetLengs:\n",
    "        return array\n",
    "    # 長さ100を長さ3にするとしたら、25, 50, 75番目を選んでる\n",
    "    elif len(array)> targetLengs:\n",
    "        targetIndexTips = len(array) / (targetLengs + 1)\n",
    "        k = 1\n",
    "        targetIndex = int(targetIndexTips * k)\n",
    "        for i in range(targetLengs):\n",
    "            targetArray[i] =array[targetIndex]\n",
    "            k += 1\n",
    "            targetIndex = int(targetIndexTips * k)\n",
    "    # 長さ13から長さ16への変換でデータを3個追加するとしたら、13を(3+1)等分した箇所にデータを追加する\n",
    "    elif len(array) < targetLengs:\n",
    "        diff = targetLengs - len(array)\n",
    "        targetIndexTips = targetLengs / (diff + 1)\n",
    "        k = 1\n",
    "        targetIndex = int(targetIndexTips * k)\n",
    "        j = 0\n",
    "        for i in range(targetLengs):\n",
    "            if i == targetIndex:\n",
    "                if j == 0:\n",
    "                    targetArray[i] = array[j]\n",
    "                else:\n",
    "                    targetArray[i] = (array[j-1] + array[j]) / 2\n",
    "                k += 1\n",
    "                targetIndex = int(targetIndexTips * k);\n",
    "            else:\n",
    "                targetArray[i] = array[j]\n",
    "                j += 1\n",
    "    return targetArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbecdf5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# numpyにして高速化\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "leng = 16\n",
    "\n",
    "np_zurasi = np.array([-7, -6, -5, -4, -3, -2, -1, 0, 1, 2])\n",
    "zurasi_bai = np_zurasi.shape[0]\n",
    "\n",
    "noise_bai = 5 #ノイズでn倍\n",
    "\n",
    "dlen = 6*leng #ノイズデータのデータ長\n",
    "mean = 0.0    #ノイズの平均値\n",
    "std  = 0.1    #ノイズの分散\n",
    "\n",
    "\n",
    "np_ax = df['AX'].to_numpy()\n",
    "np_ay = df['AY'].to_numpy()\n",
    "np_az = df['AZ'].to_numpy()\n",
    "np_rx = df['RX'].to_numpy()\n",
    "np_ry = df['RY'].to_numpy()\n",
    "np_rz = df['RZ'].to_numpy()\n",
    "\n",
    "np_kindsSumaho = df['kindsSumaho'].to_numpy()\n",
    "np_orientation = df['orientation'].to_numpy()\n",
    "np_kindsTataki = df['kindsTataki'].to_numpy()\n",
    "\n",
    "# n個を持った配列を用意しておくと早くなる(しておかないとめっちゃ遅い)\n",
    "data_num = df.shape[0] * zurasi_bai * noise_bai\n",
    "mynps = np.empty((data_num, leng, 6))\n",
    "\n",
    "print('df.shape[0]:', df.shape[0])\n",
    "data_num_count = 0\n",
    "for idx in range(df.shape[0]):\n",
    "# for idx in range(1):\n",
    "    if(np_kindsTataki[idx] != np_kindsTataki[idx - 1]):\n",
    "        print(idx, np_kindsSumaho[idx], np_orientation[idx], np_kindsTataki[idx])\n",
    "    \n",
    "    # 長さを変える\n",
    "    ax = np_ax[idx].split(',')\n",
    "    ax = list(map(lambda x: float(x), ax))\n",
    "    ax = getArrayLengsChange(ax, leng)\n",
    "#     ax = preprocessing.scale(ax)\n",
    "    \n",
    "    ay = np_ay[idx].split(',')\n",
    "    ay = list(map(lambda x: float(x), ay))\n",
    "    ay = getArrayLengsChange(ay, leng)\n",
    "#     ay = preprocessing.scale(ay)\n",
    "    \n",
    "    az = np_az[idx].split(',')\n",
    "    az = list(map(lambda x: float(x), az))\n",
    "    az = getArrayLengsChange(az, leng)\n",
    "#     az = preprocessing.scale(az)\n",
    "    \n",
    "    rx = np_rx[idx].split(',')\n",
    "    rx = list(map(lambda x: float(x), rx))\n",
    "    rx = getArrayLengsChange(rx, leng)\n",
    "#     rx = preprocessing.scale(rx)\n",
    "    \n",
    "    ry = np_ry[idx].split(',')\n",
    "    ry = list(map(lambda x: float(x), ry))\n",
    "    ry = getArrayLengsChange(ry, leng)\n",
    "#     ry = preprocessing.scale(ry)\n",
    "    \n",
    "    rz = np_rz[idx].split(',')\n",
    "    rz = list(map(lambda x: float(x), rz))\n",
    "    rz = getArrayLengsChange(rz, leng)\n",
    "#     rz = preprocessing.scale(rz)\n",
    "    \n",
    "    # 加速度と角速度ごとに標準化\n",
    "    ax = np.array(ax)\n",
    "    ay = np.array(ay)\n",
    "    az = np.array(az)\n",
    "    rx = np.array(rx)\n",
    "    ry = np.array(ry)\n",
    "    rz = np.array(rz)\n",
    "    \n",
    "    ax = ax.reshape(-1, 1)\n",
    "    ay = ay.reshape(-1, 1)\n",
    "    az = az.reshape(-1, 1)\n",
    "    rx = rx.reshape(-1, 1)\n",
    "    ry = ry.reshape(-1, 1)\n",
    "    rz = rz.reshape(-1, 1)\n",
    "    \n",
    "    a_ = np.concatenate([ax, ay, az], 1)\n",
    "    r_ = np.concatenate([rx, ry, rz], 1)\n",
    "    \n",
    "    \n",
    "    a_shape = a_.shape\n",
    "    r_shape = r_.shape\n",
    "    \n",
    "    a_ = a_.reshape(-1, 1)\n",
    "    r_ = r_.reshape(-1, 1)\n",
    "    \n",
    "    a_ = preprocessing.scale(a_)\n",
    "    r_ = preprocessing.scale(r_)\n",
    "    \n",
    "    a_ = a_.reshape(a_shape)\n",
    "    r_ = r_.reshape(r_shape)\n",
    "    \n",
    "    # 結合\n",
    "    np_ = np.concatenate([a_, r_], 1)\n",
    "    \n",
    "    # 前後にずらす\n",
    "    for roll_len in np_zurasi:\n",
    "        if roll_len > 0:\n",
    "            np_roll = np_[:-roll_len]\n",
    "            np_roll = np.concatenate([np.zeros([roll_len, 6]), np_roll], axis=0)\n",
    "        elif roll_len == 0:\n",
    "            np_roll = np_\n",
    "        else:\n",
    "            np_roll = np_[-roll_len:]\n",
    "            np_roll = np.concatenate([np_roll, np.zeros([abs(roll_len), 6])], axis=0)\n",
    "            \n",
    "        # ノイズを重ねる\n",
    "        for i in range(noise_bai):\n",
    "            np_noise = np.random.normal(mean, std, dlen).reshape([-1, 6])\n",
    "            np_noise = np_roll + np_noise\n",
    "\n",
    "            # numpyに追加\n",
    "            mynps[data_num_count] = np_noise\n",
    "            \n",
    "            # 表示\n",
    "#             columns = ['AX', 'AY', 'AZ', 'RX', 'RY', 'RZ']\n",
    "#             title = np_kindsSumaho[idx] + ' ' + np_orientation[idx] + ' ' + np_kindsTataki[idx]\n",
    "#             tmp_df = pd.DataFrame(data=np_noise, columns=columns, dtype='float')\n",
    "#             tmp_df.plot(title=title, style='.-')\n",
    "            \n",
    "            # index\n",
    "            data_num_count += 1\n",
    "    \n",
    "# センサデータ\n",
    "print(mynps.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f3ae4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kindsSumaho_list = []\n",
    "orientation_list = []\n",
    "kindsTataki_list = []\n",
    "\n",
    "n_bai = zurasi_bai * noise_bai\n",
    "    \n",
    "for index, df_ in df.iterrows():\n",
    "# for df_ in df.itertuples():\n",
    "    print(index, df_['kindsSumaho'], df_['orientation'], df_['kindsTataki'])\n",
    "    \n",
    "    for i in range(n_bai):\n",
    "        # listに追加\n",
    "        if df_['kindsSumaho'] == 'SE3':\n",
    "            kindsSumaho_list.append(0)\n",
    "        elif df_['kindsSumaho'] == 'Nexus9':\n",
    "            kindsSumaho_list.append(1)\n",
    "        elif df_['kindsSumaho'] == 'FireHD10':\n",
    "            kindsSumaho_list.append(2)\n",
    "\n",
    "        if df_['orientation'] == 'vertical':\n",
    "            orientation_list.append(0)\n",
    "        elif df_['orientation'] == 'horizontal':\n",
    "            orientation_list.append(1)\n",
    "\n",
    "        if df_['kindsTataki'] == 'tatakiTopRight':\n",
    "            kindsTataki_list.append(0)\n",
    "        elif df_['kindsTataki'] == 'tatakiBottomRight':\n",
    "            kindsTataki_list.append(1)\n",
    "        elif df_['kindsTataki'] == 'tatakiTopLeft':\n",
    "            kindsTataki_list.append(2)\n",
    "        elif df_['kindsTataki'] == 'tatakiBottomLeft':\n",
    "            kindsTataki_list.append(3)\n",
    "        elif df_['kindsTataki'] == 'tatakiMisDetectioin':\n",
    "            kindsTataki_list.append(4)\n",
    "        elif df_['kindsTataki'] == 'tatakiHorizontallyRight':\n",
    "            kindsTataki_list.append(5)\n",
    "        elif df_['kindsTataki'] == 'tatakiHorizontallyLeft':\n",
    "            kindsTataki_list.append(6)\n",
    "        elif df_['kindsTataki'] == 'tatakiHorizontallyMisDetectioin':\n",
    "            kindsTataki_list.append(7)\n",
    "\n",
    "        \n",
    "# 機種、向き、叩きの種類のデータ\n",
    "kindsSumaho_list = np.array(kindsSumaho_list)\n",
    "orientation_list = np.array(orientation_list)\n",
    "kindsTataki_list = np.array(kindsTataki_list)\n",
    "\n",
    "print(kindsSumaho_list.shape)\n",
    "print(orientation_list.shape)\n",
    "print(kindsTataki_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a64d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "kindsSumaho_onehot = to_categorical(kindsSumaho_list)\n",
    "orientation_onehot = to_categorical(orientation_list)\n",
    "kindsTataki_onehot = to_categorical(kindsTataki_list)\n",
    "\n",
    "kindsSumaho_onehot.shape\n",
    "orientation_onehot.shape\n",
    "kindsTataki_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36a9f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tatakiAllDatas = {\n",
    "    'tataki_data': mynps,\n",
    "    'orientation_onehot': orientation_onehot,\n",
    "    'kindsSumaho_onehot': kindsSumaho_onehot,\n",
    "    'kindsTataki_onehot': kindsTataki_onehot\n",
    "}\n",
    "\n",
    "print(tatakiAllDatas['tataki_data'].shape)\n",
    "print(tatakiAllDatas['orientation_onehot'].shape)\n",
    "print(tatakiAllDatas['kindsSumaho_onehot'].shape)\n",
    "print(tatakiAllDatas['kindsTataki_onehot'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544814c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "\n",
    "dt_now = datetime.datetime.now()\n",
    "dt_now = dt_now.strftime('%Y-%m-%d-%H-%M-%S')\n",
    "file_name = 'tatakiAllDatas' + dt_now + '.pickle'\n",
    "print(file_name)\n",
    "\n",
    "with open(file_name, 'wb') as web:\n",
    "    pickle.dump(tatakiAllDatas , web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5fa40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 100*24*n_bai\n",
    "print(test_size)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test, muki_train, muki_test, kishu_train, kishu_test = train_test_split(tatakiAllDatas['tataki_data'],\n",
    "                                                                                                    tatakiAllDatas['kindsTataki_onehot'],\n",
    "                                                                                                    tatakiAllDatas['orientation_onehot'],\n",
    "                                                                                                    tatakiAllDatas['kindsSumaho_onehot'],\n",
    "                                                                                                    test_size=test_size,\n",
    "                                                                                                    shuffle=True,\n",
    "                                                                                                    random_state=0,\n",
    "                                                                                                    stratify=tatakiAllDatas['kindsTataki_onehot']\n",
    "                                                                                                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ed615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(muki_train.shape)\n",
    "print(kishu_train.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(muki_test.shape)\n",
    "print(kishu_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a605f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "inputs   = tf.keras.layers.Input(shape=x_train.shape[1:], name='inputs_waveform')\n",
    "\n",
    "inputs_muki   = tf.keras.layers.Input(shape=muki_train.shape[1:], name='inputs_orientation')\n",
    "# relu_muki     = tf.keras.layers.Dense(64, activation='tanh', name='relu_orientation')(inputs_muki)\n",
    "relu_muki     = tf.keras.layers.Dense(8, activation='tanh', name='relu_orientation')(inputs_muki)\n",
    "\n",
    "inputs_kishu   = tf.keras.layers.Input(shape=kishu_train.shape[1:], name='inputs_kinds')\n",
    "# relu_kishu     = tf.keras.layers.Dense(64, activation='tanh', name='relu_kinds')(inputs_kishu)\n",
    "relu_kishu     = tf.keras.layers.Dense(8, activation='tanh', name='relu_kinds')(inputs_kishu)\n",
    "\n",
    "# gru1     = tf.keras.layers.GRU(64, return_sequences=False, name='gru1')(inputs)\n",
    "# dropout1 = tf.keras.layers.Dropout(0.1, name='dropout1')(gru1)\n",
    "\n",
    "lstm1    = tf.keras.layers.LSTM(64, return_sequences=False)(inputs)\n",
    "dropout2 = tf.keras.layers.Dropout(0.1)(lstm1)\n",
    "\n",
    "rnn1     = tf.keras.layers.SimpleRNN(64, return_sequences=False)(inputs)\n",
    "dropout3 = tf.keras.layers.Dropout(0.1)(rnn1)\n",
    "\n",
    "# relu1     = tf.keras.layers.SimpleRNN(64, return_sequences=False)(inputs)\n",
    "# dropout4     = tf.keras.layers.Dropout(0.1)(xxx1)\n",
    "\n",
    "# relu1    = tf.keras.layers.Dense(64, activation='relu', name='relu')(inputs)\n",
    "# dropout4 = tf.keras.layers.Dropout(0.1)(relu1)\n",
    "\n",
    "concatenate1 = tf.keras.layers.Concatenate()([dropout2, dropout3, relu_muki, relu_kishu])\n",
    "# concatenate1 = tf.keras.layers.Concatenate()([dropout2, dropout3])\n",
    "\n",
    "relu2    = tf.keras.layers.Dense(64, activation='relu', name='relu1')(concatenate1)\n",
    "# # dropout5 = tf.keras.layers.Dropout(0.1)(relu2)\n",
    "# dropout5 = tf.keras.layers.Dropout(0.2)(relu2)\n",
    "# relu3    = tf.keras.layers.Dense(64, activation='relu', name='relu2')(dropout5)\n",
    "\n",
    "# # concatenate2 = tf.keras.layers.Concatenate()([relu2, relu3])\n",
    "# concatenate2 = tf.keras.layers.Concatenate()([dropout5, relu3])\n",
    "\n",
    "# relu4    = tf.keras.layers.Dense(64, activation='relu', name='relu3')(concatenate2)\n",
    "# dropout6 = tf.keras.layers.Dropout(0.1)(relu4)\n",
    "\n",
    "# outputs  = tf.keras.layers.Dense( y_train.shape[-1] , activation='softmax', name='softmax')(dropout6)\n",
    "# outputs  = tf.keras.layers.Dense( y_train.shape[-1] , activation='softmax', name='softmax')(dropout5)\n",
    "outputs  = tf.keras.layers.Dense( y_train.shape[-1] , activation='softmax', name='softmax')(relu2)\n",
    "\n",
    "\n",
    "# 入出力を定義します。\n",
    "model    = tf.keras.models.Model(inputs=[inputs, inputs_muki, inputs_kishu], outputs=outputs)\n",
    "\n",
    "\n",
    "\n",
    "# opt = tf.optimizers.Adam(lr=0.1)\n",
    "# model.compile(optimizer=opt, loss='mse')\n",
    "model.compile(\n",
    "                optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# model.fit(x_train, y_train, epochs=100, batch_size=1, verbose=0)\n",
    "history = model.fit([x_train, muki_train, kishu_train], y_train,\n",
    "                    validation_data = ([x_test, muki_test, kishu_test], y_test),\n",
    "                    batch_size=32,\n",
    "                    epochs=32,\n",
    "#                     batch_size=64,\n",
    "#                     epochs=16,\n",
    "#                     batch_size=16,\n",
    "#                     epochs=64,\n",
    "                    verbose=1)\n",
    "\n",
    "# print(model.predict([x_test, muki_test, kishu_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d388803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate([x_test, muki_test, kishu_test], y_test)\n",
    "print(score[0])\n",
    "print(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd33b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_prob = model.predict([x_test, muki_test, kishu_test])\n",
    "\n",
    "predict_classes = np.argmax(predict_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71e0bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "labels = ['vertically/topRight', 'vertically/bottomRight', 'vertically/topLeft', 'vertically/bottomLeft', 'vertically/misDetectioin',\n",
    "          'horizontally/topRight', 'horizontally/bottomRight', 'horizontally/misDetectioin']\n",
    "\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "y_pred = predict_classes\n",
    "cm = confusion_matrix(y_true, y_pred)#, labels=labels) \n",
    "\n",
    "columns_labels = [\"pred_\" + str(l) for l in labels]\n",
    "index_labels = [\"act_\" + str(l) for l in labels]\n",
    "cm = pd.DataFrame(cm, columns=columns_labels, index=index_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea21b7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 6))\n",
    "sns.heatmap(cm, square=True, cbar=True, annot=True, fmt='d', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8161436",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['loss', 'accuracy']  # 使用する評価関数を指定\n",
    " \n",
    "plt.figure(figsize=(10, 5))  # グラフを表示するスペースを用意\n",
    " \n",
    "for i in range(len(metrics)):\n",
    " \n",
    "    metric = metrics[i]\n",
    " \n",
    "    plt.subplot(1, 2, i+1)  # figureを1×2のスペースに分け、i+1番目のスペースを使う\n",
    "    plt.title(metric)  # グラフのタイトルを表示\n",
    "    \n",
    "    plt_train = history.history[metric]  # historyから訓練データの評価を取り出す\n",
    "    plt_test = history.history['val_' + metric]  # historyからテストデータの評価を取り出す\n",
    "    \n",
    "    plt.plot(plt_train, label='training')  # 訓練データの評価をグラフにプロット\n",
    "    plt.plot(plt_test, label='validation')  # テストデータの評価をグラフにプロット\n",
    "    plt.legend()  # ラベルの表示\n",
    "    \n",
    "plt.show()  # グラフの表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe4db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_now = datetime.datetime.now()\n",
    "dt_now = dt_now.strftime('%Y-%m-%d-%H-%M-%S')\n",
    "my_model_path = 'tatakimodel' + dt_now + '.h5'\n",
    "print(my_model_path)\n",
    "\n",
    "model.save(my_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36896878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "\n",
    "dt_now = datetime.datetime.now()\n",
    "dt_now = dt_now.strftime('%Y-%m-%d-%H-%M-%S')\n",
    "tfjs_target_dir = './tfjs' + dt_now\n",
    "print(tfjs_target_dir)\n",
    "\n",
    "tfjs.converters.save_keras_model(model, tfjs_target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38fb19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "a = accuracy_score(y_true, y_pred)\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ccc95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
